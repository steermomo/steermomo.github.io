Title: åœ¨NCCLåç«¯ä¸‹Pytorchçš„distributed.all_gatherå¡æ­»æ’æŸ¥
Date: 2021-01-05 21:20
Modified: 2021-01-05 21:20
Category: Coding
Tags: PyTorch, Python
Slug: pytorch-dist-nccl-backend-allgather-stuck

ç”¨äº†Githubä¸Šä¸€ä¸ª[SimCLRçš„PyTorchå®ç°](https://github.com/Spijkervet/SimCLR)ï¼Œä½†æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°äº†ä¸€äº›é—®é¢˜ã€‚

åŸrepoè¦ç”¨DDPè®­ç»ƒçš„æ–¹å¼æœ‰ç‚¹è´¨æœ´ï¼Œéœ€è¦æ‰‹åŠ¨å¯åŠ¨Nçš„è¿›ç¨‹è¿›è¡Œè®­ç»ƒï¼Œé‚£8å¼ å¡å²‚ä¸æ˜¯è¦æ“ä½œå…«æ¬¡ï¼å®éªŒå¼€å¤šäº†è¦ç´¯æ­»äº†ã€‚
```bash
CUDA_VISIBLE_DEVICES=0 python main.py --nodes 2 --nr 0
CUDA_VISIBLE_DEVICES=1 python main.py --nodes 2 --nr 1
CUDA_VISIBLE_DEVICES=2 python main.py --nodes 2 --nr 2
CUDA_VISIBLE_DEVICES=N python main.py --nodes 2 --nr 3
```

## ä¸€è¡Œä»£ç èµ·

è¿™è‚¯å®šä¸èƒ½å¿ï¼Œä¸ç¬¦åˆæˆ‘çš„é£æ ¼ï¼Œæˆ‘ä»é™ˆå¹´ä»£ç åŒ…é‡Œæ‹¿å‡ºäº†æˆ‘çš„é¡ºæ‰‹å·¥å…·ï¼Œèƒ½ä¸€è¡Œåšå®Œçš„äº‹æƒ…ç»ä¸ç”¨å…«è¡Œã€‚
```bash

python3 -m torch.distributed.launch --nproc_per_node 2 --master_port=9495 main.py
```

ä¸€è¡Œä»£ç èµ·ï¼å®Œç¾ï¼

åé¢è¿˜æœ‰ä¸€äº›æŠŠåˆ†å¸ƒå¼é€šè®¯çš„é…ç½®ä¿®æ”¹çš„é—®é¢˜ï¼Œæ¯”å¦‚ç§»é™¤
```python
# Master address for distributed data parallel
os.environ["MASTER_ADDR"] = "127.0.0.1"
os.environ["MASTER_PORT"] = "8000"
mp.spawn(main, args=(args,), nprocs=args.gpus, join=True)
```
åŠ ä¸Š
```python
dist.init_process_group(backend='nccl', rank=args.local_rank)
```

## Debugé Google

å®Œäº‹äº†ï¼ŒDockerè°ƒèµ·ã€ç£ç›˜æŒ‚è½½ã€ç¨‹åºå¯åŠ¨...

ç„¶åå°±çœ‹è§GPU0å¡æ­»100%ï¼ŒGPU1-Nå‚»ç«™ç€ä¸åŠ¨ã€‚

è¿™ä¸è¡Œï¼Œä¸Šå·¥å…·å®šä½ï¼Œå‘ç°æ˜¯å¤šå¡é€šè®¯ä¸­å¡åœ¨dist.all_gatherè¿™ä¸€æ­¥ã€‚
è¿™é‡Œæœ‰ä¸€æ­¥å®ç°éœ€è¦å¯¹æ‰€æœ‰å¡ä¸Šçš„ç‰¹å¾è¿›è¡Œèšåˆè®¡ç®—æ­£è´Ÿæ ·æœ¬ï¼Œä½†åªå¸Œæœ›ä¿ç•™å½“å‰å¡å†…çš„æ¢¯åº¦ã€‚
```python
class GatherLayer(torch.autograd.Function):
    '''Gather tensors from all process, supporting backward propagation.
    '''

    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        output = [torch.zeros_like(input) \
            for _ in range(dist.get_world_size())]

        dist.all_gather(output, input)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grads):
        input, = ctx.saved_tensors
        grad_out = torch.zeros_like(input)
        grad_out[:] = grads[dist.get_rank()]
        return grad_out
```


æƒ³ä¸‹ç­å›å®¶äº†...ä¸æƒ³å†™äº†ï¼Œåæ­£å°±æ˜¯å¯¹ç€Googleä¸€é¡¿çŒ›æœï¼Œå‘ç°æœ‰è·Ÿæˆ‘ä¸€æ ·çš„å€’éœ‰è›‹é‡åˆ°äº†ç±»ä¼¼çš„é—®é¢˜[^1]ï¼Œå°±æ˜¯åœ¨NCCLåˆå§‹åŒ–init_process_groupä¹‹å‰ï¼Œéœ€è¦å…ˆä¸ºæ¯ä¸ªè¿›ç¨‹åˆ†é…GPUï¼Œåˆ†é…å®Œäº‹äº†å°±æ²¡ä»»ä½•é—®é¢˜äº†ã€‚
```python
torch.cuda.set_device(args.rank)
dist.init_process_group(backend='nccl', rank=local_rank)
```



AP:
ğŸ¤£

ä¹‹å‰è¿˜é‡åˆ°è¿‡è¯¡å¼‚çš„M40å¡æ­»åœ¨dist.all_gatherï¼Œä½†æ˜¯åœ¨P40ä¸Šè¿è¡Œå°±ååˆ†æ­£å¸¸ï¼Œè¿˜æ‰¾äº†æœºå™¨å­¦ä¹ å¹³å°çš„åŒäº‹èŠ±äº†ä¸€ä¸‹åˆæ—¶é—´debugã€‚

ç°åœ¨å›å»æ£€æŸ¥å½“æ—¶å‡ºç°è¯¡å¼‚bugçš„ä»£ç ï¼Œæœç„¶æ˜¯æˆ‘å†™é”™äº†ï¼Œæˆ‘ç»™è¿™ä¸¤å¥å†™åäº†å•Šï¼ğŸ˜‘ğŸ™„
```python
dist.init_process_group(backend='nccl', rank=local_rank)
torch.cuda.set_device(args.local_rank)
```


[^1]: [distributed.all_gather function stuck when using NCCL backend](https://github.com/pytorch/pytorch/issues/18689)