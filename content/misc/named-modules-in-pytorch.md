Title: Pytorchä¸­çš„named_modulesæ˜¯ä¸ªå•¥-debugçœŸéš¾
Date: 2019-09-05 13:20
Modified: 2019-09-05 21:20
Category: Pytorch,
Tags: Python, Pytorch
Slug: named-modules-in-pytorch
Summary: 


Summary =>  åœ¨æ”¹å†™deeplabv3+æ—¶, å› ä¸ºéœ€è¦å»æ‰ASPPå’Œdecoderéƒ¨åˆ†, ä»…ä¿ç•™mobilenetçš„backbone. ä½†æ˜¯å› ä¸ºè¯¯å¯¹`nn.Module`å†…çš„æ‰€æœ‰æƒå€¼è¿›è¡Œåˆå§‹åŒ–, å¯¼è‡´backboneä¸­çš„é¢„è®­ç»ƒæƒå€¼ä¸¢å¤±. åŒæ ·çš„æ¨¡å‹è®­ç»ƒ1000ä¸ªepochçš„æ•ˆæœä¾ç„¶å¾ˆå·®.

<img src="{static}/images/file_3938986.jpg" style="max-width: 30%">

è¿™å‡ å¤©é‡åˆ°äº†ä¸€ä¸ªååˆ†å¥‡æ€ªçš„é—®é¢˜, å®éªŒéœ€è¦ç”¨åˆ°mobilenetv2ä½œä¸ºbackboneçš„deeplabv3+åšåˆ†å‰², åœ¨å¯¹æ¯”å®éªŒä¸­éœ€è¦ä¸¢æ‰ASPPå’Œdecoderéƒ¨åˆ†, å³backboneåé¢ç›´æ¥æ¥ä¸Šä¸¤å±‚å·ç§¯.  

æœ€å¼€å§‹æ˜¯ç›´æ¥åœ¨åŸæ¥çš„deeplabv3+çš„ä»£ç ä¸Šé­”æ”¹, å³åˆ›å»ºæ–°çš„ASPPç»“æ„ç›´æ¥è¿”å›input, å†åˆ›å»ºæ–°çš„decoderéƒ¨åˆ†åªæ¥å—high level featureä½œä¸ºè¾“å‡º. 


ä½†æ˜¯è¿™æ ·ä¿®æ”¹çš„æ¨¡å‹, åœ¨æµ‹è¯•é€Ÿåº¦æ—¶æ¯”åŸå§‹çš„MobileNetV2è¦æ…¢å¾ˆå¤š, æ²¡æ³•ç»™å‡ºæ­£ç¡®çš„æ€§èƒ½æ•°æ®. ä½†æ˜¯åŸå§‹çš„MobileNetåˆå¥½åƒæ²¡æœ‰ç›´æ¥æŒ‡å®š`out_stride`çš„åœ°æ–¹. æˆ‘å°±æŒ‰ç…§ä¿®æ”¹åçš„ASPPè·Ÿdecoderéƒ¨åˆ†, åœ¨deeplabv3+çš„backboneåé¢åŠ äº†ä¸¤å±‚å·ç§¯. æµ‹å®Œé€Ÿåº¦, æ•ˆæœè¾¾åˆ°, ååˆ†å¼€å¿ƒ.


åœ¨åç»­è·‘å®éªŒçš„è¿‡ç¨‹ä¸­, å‘ç°åé¢å†™çš„è¿™ä¸ªæ¨¡å‹, è™½ç„¶èƒ½è®­ç»ƒ, èƒ½æ”¶æ•›, æ•ˆæœå¥‡å·®.

åé¢çš„æ¨¡å‹, å®šä¹‰æ›´åŠ ç®€å•, åœ¨ç»“æ„ä¸Šæ›´æ˜¯ä¸€æ¨¡ä¸€æ ·, ç”¨torchsummaryè¾“å‡ºä¸¤ä¸ªæ¨¡å‹çš„ç»“æ„, æ›´æ˜¯ä¸€ç‚¹æ²¡å·®, ä½œä¸ºå¯¹æ¯”, ä½¿ç”¨äº†åŒä¸€ä»½è®­ç»ƒä»£ç , å®Œå…¨ç›¸åŒçš„è®­ç»ƒå‚æ•°, åè€…çš„æ•ˆæœä»ç„¶æ˜¯æ— é™é€¼è¿‘ä½†æ°¸è¿œè¾¾ä¸åˆ°å‰è€…10ä¸ªepochçš„æ•ˆæœ. ä¸æ˜¯ç„å­¦å°±æ˜¯Bug.


åŸå§‹çš„deeplabv3+çš„ä»£ç å®šä¹‰æ˜¯è¿™æ ·çš„
```python
class DeepLab(nn.Module):
    def __init__(self, backbone='resnet', output_stride=16, num_classes=21,
                 sync_bn=True, freeze_bn=False, deeplab_header=True):
        super(DeepLab, self).__init__()

        self.backbone = build_backbone(backbone, output_stride, BatchNorm)
        self.aspp = build_aspp(backbone, output_stride, BatchNorm, deeplab_header)
        self.decoder = build_decoder(num_classes, backbone, BatchNorm, deeplab_header)

        if freeze_bn:
            self.freeze_bn()

    def forward(self, input):
        x, low_level_feat = self.backbone(input)
        x = self.aspp(x)
        x = self.decoder(x, low_level_feat)
        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)
        return x

    def freeze_bn(self):
        for m in self.modules():
            if isinstance(m, SynchronizedBatchNorm2d):
                m.eval()
            elif isinstance(m, nn.BatchNorm2d):
                m.eval()

    def get_1x_lr_params(self):
        modules = [self.backbone]
        for i in range(len(modules)):
            for m in modules[i].named_modules():
                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                        or isinstance(m[1], nn.BatchNorm2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p

    def get_10x_lr_params(self):
        modules = [self.aspp, self.decoder]
        for i in range(len(modules)):
            for m in modules[i].named_modules():
                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                        or isinstance(m[1], nn.BatchNorm2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p
```

ä¿®æ”¹ä¹‹åçš„ä»£ç ç›´æ¥å°†ASPPå’Œdecoderçš„å†…å®¹æ‹¿äº†å‡ºæ¥.

```python
class MobilenetV2(nn.Module):
    def __init__(self, output_stride=8, num_classes=2,sync_bn=True, freeze_bn=False):
        super(MobilenetV2, self).__init__()
        if sync_bn == True:
            BatchNorm = SynchronizedBatchNorm2d
        else:
            BatchNorm = nn.BatchNorm2d

        self.feat_conv = nn.Sequential(
            nn.Conv2d(320, 256, kernel_size=3, stride=1, padding=1, bias=False),
            BatchNorm(256),
            nn.ReLU()
        )
        self.prob_conv = nn.Sequential(
            nn.Dropout(0.1),
            nn.Conv2d(256, num_classes, kernel_size=1, stride=1, padding=0)
        )
        self._init_weight()

    def forward(self, x):
        mob_feat, low_level_feat = self.backbone(x)
        last_feat = self.feat_conv(mob_feat)
        prob_map = self.prob_conv(last_feat)
        ret = F.interpolate(prob_map, size=x.size()[2:], mode='bilinear', align_corners=True)
        return ret
    
    def get_1x_lr_params(self):
        modules = [self.backbone]
        for i in range(len(modules)):
            for m in modules[i].named_modules():
                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                        or isinstance(m[1], nn.BatchNorm2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p

    def get_10x_lr_params(self):
        modules = [self.feat_conv, self.prob_conv]
        for i in range(len(modules)):
            for m in modules[i].named_modules():
                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], SynchronizedBatchNorm2d) \
                        or isinstance(m[1], nn.BatchNorm2d):
                    for p in m[1].parameters():
                        if p.requires_grad:
                            yield p

    def _init_weight(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, SynchronizedBatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
```


è¿™é‡Œå”¯ä¸€æ˜¾è‘—çš„æ”¹å˜å°±æ˜¯å°†ASPPå’Œdecoderçš„å†…å®¹æ‹¿äº†å‡ºæ¥.

<br>
### named_modules
> Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.

åœ¨torch.nn.modules.moduleçš„æºç ä¸­, named_modulesçš„å®ç°æ˜¯è¿™æ ·çš„
```python
def named_modules(self, memo=None, prefix=''):
    if memo is None:
        memo = set()
    if self not in memo:
        memo.add(self)
        yield prefix, self
        for name, module in self._modules.items():
            if module is None:
                continue
            submodule_prefix = prefix + ('.' if prefix else '') + name
            for m in module.named_modules(memo, submodule_prefix):
                yield m
```
å¯ä»¥çœ‹å‡ºæ¥è¿™ä¸ªè°ƒç”¨è¿‡ç¨‹æ˜¯é€’å½’è°ƒç”¨, é‚£æˆ‘æŠŠdecoderå†…çš„è¿ç®—ç”¨`nn.Module`åŒ…è£…èµ·æ¥å’Œæ‹¿åˆ°å¤–é¢åº”è¯¥æ²¡æœ‰åŒºåˆ«, æ€»ä¼šé€’å½’åˆ°æ‰€æœ‰çš„æˆå‘˜.


åœ¨Moduleç±»ä¸­, å¯ä»¥çœ‹åˆ°_modulesæ˜¯ä¸€ä¸ªæœ‰åºå­—å…¸.
```python
def _construct(self):
    """
    Initializes internal Module state, shared by both nn.Module and ScriptModule.
    """
    torch._C._log_api_usage_once("python.nn_module")
    self._backend = thnn_backend
    self._parameters = OrderedDict()
    self._buffers = OrderedDict()
    self._backward_hooks = OrderedDict()
    self._forward_hooks = OrderedDict()
    self._forward_pre_hooks = OrderedDict()
    self._state_dict_hooks = OrderedDict()
    self._load_state_dict_pre_hooks = OrderedDict()
    self._modules = OrderedDict()
```

emm...å½“æˆ‘å‘ç°`named_modules()`æ˜¯é€’å½’è°ƒç”¨çš„æ—¶å€™, æˆ‘è¿˜æ²¡æœ‰æ„è¯†åˆ°æ˜¯å“ªé‡Œå‡ºäº†é—®é¢˜, å½“æˆ‘çœ‹åˆ°`modules()`å®é™…ä¸Šæ˜¯è°ƒç”¨äº†`named_modules()`çš„æ—¶å€™,ğŸ™„.  


æ‰€ä»¥é—®é¢˜æ˜¯æˆ‘åœ¨æ”¹å†™äº†åŸæ¥deeplabv3+ä¹‹å, å› ä¸ºå°†ASPPå’Œdecoderçš„å†…å®¹éƒ½ç›´æ¥æ˜¾ç¤ºåœ°å†™åœ¨`forward()`æ–¹æ³•ä¸­, éœ€è¦å¯¹`feat_conv`å’Œ`prob_conv`åˆå§‹åŒ–, åˆå§‹åŒ–æ–¹æ³•ä¸­è°ƒç”¨äº†`modules()`éå†æ‰€æœ‰å­`Conv2d`, åˆå§‹åŒ–å…¶æƒé‡, æ‰€ä»¥å¯¼è‡´`backbone`ä¸­çš„æ‰€æœ‰æƒå€¼éƒ½è¢«åˆå§‹åŒ–äº†ğŸ˜¤. ä¸¢äº†é¢„è®­ç»ƒæƒå€¼, æ•ˆæœè‡ªç„¶å·®äº†ä¸€å¤§æˆª, è®­ç»ƒ1000ä¸ªepochéƒ½æŠµä¸è¿‡è®­ç»ƒ10ä¸ªepochçš„æ•ˆæœ.


æŠŠæƒé‡åˆå§‹åŒ–çš„éƒ¨åˆ†æ”¹ä¸ºè·³è¿‡backboneéƒ¨åˆ†, é—®é¢˜è§£å†³.
```python
def _init_weight(self):
    modules = [self.feat_conv, self.prob_conv]:
    for i in range(len(modules)):
        for m in modules[i].modules():
            if isinstance(m, nn.Conv2d):
                torch.nn.init.kaiming_normal_(m.weight)
            elif isinstance(m, SynchronizedBatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()
```

Slide note: è™½ç„¶ç½—é‡Œå·´å½è®²äº†ä¸€å †æ²¡ç”¨çš„ä¸œè¥¿, ä¸è¿‡æ˜¯åœ¨å•°å—¦çš„è¿‡ç¨‹ä¸­å‘ç°äº†Bugçš„ä½ç½®, æˆ‘ä¸€ç›´ä»¥ä¸ºæ˜¯å› ä¸ºå¥—äº†ä¸€å±‚`nn.Module`å¯¼è‡´`get_10x_lr_params()`æ–¹æ³•ä¸èƒ½æ­£ç¡®è¿”å›å‚æ•°, æ’é™¤äº†å‡ ä¸ªé—®é¢˜ä¹‹åæ‰å‘ç°Bugè¿™ä¹ˆæ˜æ˜¾. 

æˆ‘å†³å®šsave&push, åˆ¶é€ äº’è”ç½‘åƒåœ¾.