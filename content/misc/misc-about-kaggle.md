Title: Winning a kaggle Competition æ•™ä½ å¦‚ä½•å™¶éŸ­èœ
Date: 2019-08-26 18:16
Modified: 2019-08-26 18:16
Tags: kaggle, 
Slug: misc-about-kaggle
Summary: 

æˆ‘åˆè¿·ä¿¡äº†å‘€....

<img src="{static}/images/sticker_newthing.webp" style="max-width: 30%">



çœ‹äº†DataCampçš„[Winning a Kaggle Competition in Python ](https://www.datacamp.com/courses/winning-a-kaggle-competition-in-python/continue), è§‰å¾—æœ‰ç‚¹ç”¨çš„å°±æ˜¯ä»‹ç»Target encodingçš„é‚£éƒ¨åˆ†. ä¸«çš„å‹æ ¹å°±æ²¡æ•™æˆ‘æ€ä¹ˆWinning.  æˆ‘è‚¯å®šä¹Ÿæ˜¯è¢«æ”¶æ™ºå•†ç¨çš„ä¸€ç±»äºº, å¹¸å¥½è¯¾ç¨‹æ˜¯ç™½å«–çš„. Visual Studio Dev Essentials é€äº†ä¿©æœˆçš„DataCampè®¢é˜….



å”¯ä¸€è®©æˆ‘è§‰å¾—æ–°é²œçš„, å°±æ˜¯target encoding, å› ä¸ºä¹‹å‰æ²¡çœ‹è¿‡. å…¶ä»–çš„éƒ½æ˜¯æ³›æ³›è€Œè°ˆ.

éƒ½æ²¡æœ‰é“¶å¼¹çš„ä¹ˆğŸ±â€ğŸ‰


<br>

#### Target encoding

Mean target encoding

- smoothing





1. åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼, åº”ç”¨åˆ°æµ‹è¯•é›†ä¸Š
2. oof, åœ¨æŸä¸ªkfoldä¸Šè®¡ç®—, åº”ç”¨åˆ°(k-1)foldä¸Š




```Python
# åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—å‡å€¼, åœ¨æµ‹è¯•é›†ä¸Šåº”ç”¨mean target encoding
def test_mean_target_encoding(train, test, target, categorical, alpha=5):
    # Calculate global mean on the train data
    global_mean = train[target].mean()
    
    # Group by the categorical feature and calculate its properties
    train_groups = train.groupby(categorical)
    category_sum = train_groups[target].sum()
    category_size = train_groups.size()
    
    # Calculate smoothed mean target statistics
    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)
    
    # Apply statistics to the test data and fill new categories
    test_feature = test[categorical].map(train_statistics).fillna(global_mean)
    return test_feature.values


# åœ¨è®­ç»ƒé›†ä¸Šoof, åº”ç”¨mean target encoding
def train_mean_target_encoding(train, target, categorical, alpha=5):
    # Create 5-fold cross-validation
    kf = KFold(n_splits=5, random_state=123, shuffle=True)
    train_feature = pd.Series(index=train.index)
    
    # For each folds split
    for train_index, test_index in kf.split(train):
        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]
      
        # Calculate out-of-fold statistics and apply to cv_test
        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)
        
        # Save new feature for this particular fold
        train_feature.iloc[test_index] = cv_test_feature       
    return train_feature.values


#è°ƒç”¨ä¸Šè¿°ä¸¤ä¸ªå‡½æ•°, åˆ†åˆ«å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åštarget encoding
def mean_target_encoding(train, test, target, categorical, alpha=5):
  
    # Get test feature
    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)
    
    # Get train feature
    train_feature = train_mean_target_encoding(train, target, categorical, alpha)
    
    # Return new features to add to the model
    return train_feature, test_feature
```

åœ¨K-fold CVä¸­ä½¿ç”¨ target encoding
```Python
# Create 5-fold cross-validation
kf = KFold(n_splits=5, random_state=123, shuffle=True)

# For each folds split
for train_index, test_index in kf.split(bryant_shots):
    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]

    # Create mean target encoded feature
    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,
                                                                           test=cv_test,
                                                                           target='shot_made_flag',
                                                                           categorical='game_id',
                                                                           alpha=5)
    # Look at the encoding
    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))
```

<br>
### Missing Data



Numerical data  
- Mean/Median imputation
- Constant value imputation (-999)

Categorical data  
- é«˜é¢‘é¡¹   
- æ–°å€¼ (MISS)


sklearn.impute SimpleImputer
```Python
# Import SimpleImputer
from sklearn.impute import SimpleImputer

# Create mean imputer
mean_imputer = SimpleImputer(strategy='mean')

# Price imputation
rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])

# Import SimpleImputer
from sklearn.impute import SimpleImputer

# Create constant imputer
constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')

# building_id imputation
rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])
```

<br>
### Baseline Model



<br>
### Hyperparameter tuning

|     Competition type     | Feature engineering | Hyperparameter  optimization |
| :----------------------: | :-----------------: | :--------------------------: |
| Classic Machine Learning |         +++         |              +               |
|      Deep Learning       |          -          |             +++              |



- Grid search
- Random search
- Bayesian optimization



<br>

### Model ensemble



- blending
  - å¤šä¸ªæ¨¡å‹ï¼Œ å–å¹³å‡
- stacking
  - å¤šå±‚é¢„æµ‹
  - ç¬¬ä¸€å±‚æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼Œ ä½œä¸ºç¬¬äºŒå±‚æ¨¡å‹çš„è¾“å…¥ç‰¹å¾



ç®€å•çš„blending

```Python
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

# Train a Gradient Boosting model
gb = GradientBoostingRegressor().fit(train[features], train['fare_amount'])

# Train a Random Forest model
rf = RandomForestRegressor().fit(train[features], train['fare_amount'])

# Make predictions on the test data
test['gb_pred'] = gb.predict(test[features])
test['rf_pred'] = rf.predict(test[features])

# Find mean of model predictions
test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2
print(test[['gb_pred', 'rf_pred', 'blend']].head(3))
```



ç®€å•çš„stacking

```python
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

# Split train data into two parts
part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)

# Train a Gradient Boosting model on Part 1
gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)

# Train a Random Forest model on Part 1
rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)


# Make predictions on the Part 2 data
part_2['gb_pred'] = gb.predict(part_2[features])
part_2['rf_pred'] = rf.predict(part_2[features])

# Make predictions on the test data
test['gb_pred'] = gb.predict(test[features])
test['rf_pred'] = rf.predict(test[features])


from sklearn.linear_model import LinearRegression

# Create linear regression model without the intercept
lr = LinearRegression(fit_intercept=False)

# Train 2nd level model in the part_2 data
lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)

# Make stacking predictions on the test data
test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])

# Look at the model coefficients
print(lr.coef_)
```





<br>

### Tips

1. Save folds
2. Save model runs
3. Save model predictions
4. Save performance results